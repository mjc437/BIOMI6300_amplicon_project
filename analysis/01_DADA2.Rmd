---
title: "Complete: Infer ASVs with DADA2"
author: "Maya Craig"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document: 
    code_folding: show
    theme: spacelab
    highlight: pygments
    keep_md: no
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
      toc_depth: 3
  keep_md: true  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
                      fig.path = "../figures/01_DADA2/") # send any figure output to this folder
```


## Set seed 
```{r set-seed}
set.seed(4750)
```


```{r script-time}
# What time did we start running this script? 
start_time <- Sys.time()
start_time
```

# Goals of this file 

1. Use raw fastq files and generate quality plots to assess quality of reads.
2. Filter and trim out bad sequences and bases from our sequencing files. 
3. Write out fastq files with high quality sequences. 
4. Evaluate the quality from our filter and trim. 
5. Infer Errors on forward and reverse reads individually.
6. Identified ASVs on forward and reverse reads separately, using the error model.  
7. Merge forward and reverse ASVs into "contiguous ASVs".  
8. Generate the ASV count table. (`otu_table` input for phyloseq.). 


# Load packages 
```{r load-packages}
# Use this to install the most recent version of patchwork (otherwise the aggregated plot functions will not work)
#devtools::install_github("thomasp85/patchwork@HEAD")
# Efficient package loading with pacman 
pacman::p_load(tidyverse, BiocManager, devtools, dada2, 
               phyloseq, patchwork, DT, iNEXT, vegan,
               install = FALSE)
library(ggplot2)
```


# Load data 
```{r load-data}
# Set the raw fastq path to the raw sequencing files 
# Path to the fastq files 
raw_fastqs_path <- "data/01_raw_gzipped_fastqs"
raw_fastqs_path
head(list.files(raw_fastqs_path))
str(list.files(raw_fastqs_path)) #how many files 
```

# Create vector of forward reads 
```{r forward-reads-vector}
forward_reads <- list.files(raw_fastqs_path, pattern = "_1.fastq.gz", full.names = TRUE)  

#Intuition check
head(forward_reads)  
```

# Create vector of reverse reads
```{r reverse-reads-vector}
reverse_reads <- list.files(raw_fastqs_path, pattern = "_2.fastq.gz", full.names = TRUE)  
head(reverse_reads)  
```

# Asses Read Quality 

## Evaluate raw read sequence quality 

- Examining the quality of reads before trimming 

```{r raw-aggregate-plot}
# Randomly select 12 samples from dataset to evaluate the quality
random_samples <- sample(1:length(reverse_reads), size = 12)
random_samples

# Calculate and plot quality of these twelve samples (for forward and reverse reads)
forward_filteredQual_plot_12 <- plotQualityProfile(forward_reads[random_samples]) + 
  labs(title = "Forward Read Raw Quality")

reverse_filteredQual_plot_12 <- plotQualityProfile(reverse_reads[random_samples]) + 
  labs(title = "Reverse Read Raw Quality")

preQC_aggregate_plot <- forward_filteredQual_plot_12 + reverse_filteredQual_plot_12

preQC_aggregate_plot #Note: make sure the patchwork and ggplot packages are up to date for this to run!
```

# Aggregate all QC plots
```{r aggregate-QC-plots}
# Forward reads
forward_preQC_plot <- 
  plotQualityProfile(forward_reads, aggregate = TRUE) + 
  labs(title = "Forward Pre-QC")

# reverse reads
reverse_preQC_plot <- 
  plotQualityProfile(reverse_reads, aggregate = TRUE) + 
  labs(title = "Reverse Pre-QC")

preQC_aggregate_plot <- 
  # Plot the forward and reverse together 
  forward_preQC_plot + reverse_preQC_plot

# Show the plot
preQC_aggregate_plot
```

**Interpretation:** Before the filtering in trimming, the quality score or the forward reads is between 30 and 40 for each cycle up until the 200th cycle. For the reverse reads, the quality score begins to decline a little before the 250th cycle. There is a total of 3837747 reads at this point.

## Prepare a placeholder for filtered reads
```{r prep-filtered-sequences}
# vector of our samples, extract sample name from files 
samples <- sapply(strsplit(basename(forward_reads), "_"), `[`,1) 
head(samples)

# Place filtered reads into filtered_fastqs_path
filtered_fastqs_path <- "data/01_DADA2/02_filtered_fastqs"
filtered_fastqs_path

# create 2 variables: filtered_F, filtered_R
filtered_forward_reads <- 
  file.path(filtered_fastqs_path, paste0(samples, "_1_filtered.fastq.gz"))
length(filtered_forward_reads)

# reverse reads
filtered_reverse_reads <- 
  file.path(filtered_fastqs_path, paste0(samples, "_2_filtered.fastq.gz"))

# Intuition check
head(filtered_reverse_reads)
```

# Filter and Trim Reads

Parameters of filter and trim **DEPEND ON THE DATASET**. 
- The library preparation: The primers were not sequenced, therefore they do not need to be trimmed out.
- What do the above quality profiles of the reads look like? *If they are lower quality, it is highly recommended to use `maxEE = c(1,1)`.*  
- Do the reads dip suddenly in their quality? If so, explore `trimLeft` and `truncLen`

- `maxEE` is a quality filtering threshold applied to expected errors. Here, if there's 2 expected errors. It's ok. But more than 2. Throw away the sequence. Two values, first is for forward reads; second is for reverse reads.  
- `trimLeft` can be used to remove the beginning bases of a read (e.g. to trim out primers!) 
- `truncLen` can be used to trim your sequences after a specific base pair when the quality gets lower. Though, please note that this will shorten the ASVs! For example, this can be used when the quality of the sequence suddenly gets lower, or clearly is typically lower. So, if the quality of the read drops below a phred score of 25 (on the y-axis of the plotQualityProfile above, which indicates ~99.5% confidence per base).  
- `maxN` the number of N bases. Here, using ASVs, we should ALWAYS remove all Ns from the data.  

```{r filter-and-trim}
# Assign a vector to filtered reads 
# write out filtered fastq files 
# Therefore, we do not need to trim the primers, because they were not sequenced
filtered_reads <- 
  filterAndTrim(fwd = forward_reads, filt = filtered_forward_reads,
              rev = reverse_reads, filt.rev = filtered_reverse_reads, truncLen=c(250,225),
              maxN = 0, maxEE = c(2,2), trimLeft = c(19,20),
              truncQ = 2, rm.phix = TRUE, compress = TRUE, multithread = TRUE)
head(filtered_reads)
```

# Plot the 12 random samples after QC
```{r plot-random-samples}
forward_filteredQual_plot_12 <- 
  plotQualityProfile(filtered_forward_reads[random_samples]) + 
  labs(title = "Trimmed Forward Read Quality")

reverse_filteredQual_plot_12 <- 
  plotQualityProfile(filtered_reverse_reads[random_samples]) + 
  labs(title = "Trimmed Reverse Read Quality")

# Put the two plots together 
QC_aggregate_plot <- forward_filteredQual_plot_12 + reverse_filteredQual_plot_12
QC_aggregate_plot
```

## Aggregated Trimmed Plots 
```{r aggregated-plots}
# Forward reads
forward_postQC_plot <- 
  plotQualityProfile(filtered_forward_reads, aggregate = TRUE) + 
  labs(title = "Forward Post-QC")

# reverse reads
reverse_postQC_plot <- 
  plotQualityProfile(filtered_reverse_reads, aggregate = TRUE) + 
  labs(title = "Reverse Post-QC")

postQC_aggregate_plot <- 
  forward_postQC_plot + reverse_postQC_plot

# Show the plot
postQC_aggregate_plot
```

**Interpretation of trimmed plots**: The quality of both the forward and reverse read seems to have improved after filtering and trimming. For both the forward and reverse, the read quality is between 30-40.

```{r QC-stats}
# Make output into dataframe 
filtered_df <- as.data.frame(filtered_reads)
head(filtered_df)

# calculate some stats 
filtered_df %>%
  reframe(median_reads_in = median(reads.in),
          median_reads_out = median(reads.out),
          median_percent_retained = (median(reads.out)/median(reads.in)))
```
**Interpretation**: The median percent of retained reads is about 93%. There was a median of 81827 reads before quality control, and there was a median of 75805 reads after QC. This tells me that the vast majority of reads had a quality score between 30 and 40.

### Visualize QC differences in plot 
```{r pre-post-QC-plot}
# Plot the pre and post together in one plot
preQC_aggregate_plot / postQC_aggregate_plot
```


# Learn Errors

1. Starts with the assumption that the error rates are the maximum (takes the most abundant sequence ("center") and assumes it's the only sequence not caused by errors).  
2. Compares the other sequences to the most abundant sequence. 
3. Uses at most 10^8^ nucleotides for the error estimation.  
4. Uses parametric error estimation function of loses fit on the observed error rates

```{r learn-errors}
# Forward reads 
error_forward_reads <- 
  learnErrors(filtered_forward_reads, multithread = TRUE)
# Plot Forward  
forward_error_plot <- 
  plotErrors(error_forward_reads, nominalQ = TRUE) + 
  labs(title = "Forward Read Error Model")

# Reverse reads 
error_reverse_reads <- 
  learnErrors(filtered_reverse_reads, multithread = TRUE)
# Plot reverse
reverse_error_plot <- 
  plotErrors(error_reverse_reads, nominalQ = TRUE) + 
  labs(title = "Reverse Read Error Model")

# Put the two plots together
forward_error_plot + reverse_error_plot
```

**Interpretation:** It looks like the errors are a reasonable fit to the observed reads. The error rate decreases as quality increases, which makes sense.

# Infer ASVs 
```{r infer-ASVs}
# Infer ASVs on the forward sequences
dada_forward <- dada(filtered_forward_reads,
                     err = error_forward_reads, 
                     multithread = TRUE)

typeof(dada_forward)
# Grab a sample and look at it 
dada_forward$SRR23008984_1_filtered.fastq.gz


# Infer ASVs on the reverse sequences 
dada_reverse <- dada(filtered_reverse_reads,
                     err = error_reverse_reads,
                     multithread = TRUE)
# Inspect 
dada_reverse[1]
dada_reverse[30]
```

# Merge ASVs 

- The result of the merge are contig sequences
```{r merged-ASVs}
# merge forward and reverse ASVs
merged_ASVs <- mergePairs(dada_forward, filtered_forward_reads, 
                          dada_reverse, filtered_reverse_reads,
                          verbose = TRUE)

# Evaluate the output 
typeof(merged_ASVs)
length(merged_ASVs)
names(merged_ASVs)

# Inspect the merger data.frame from the 20211005-MA-CWS1P 
head(merged_ASVs[[2]])
```

# Create RAW ASV count table 
```{r raw and trimmed ASVs}
# Create the ASV Count Table 
raw_ASV_table <- makeSequenceTable(merged_ASVs)

# Write out the file to data/01_DADA2

# Check the type and dimensions of the data
dim(raw_ASV_table)
class(raw_ASV_table)
typeof(raw_ASV_table)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table)))

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Raw distribution of ASV length")

# Create the ASV Count Table 
raw_ASV_table <- makeSequenceTable(merged_ASVs)

# Write out the file to data/01_DADA2

###################################################
###################################################
# TRIM THE ASVS

# We will allow for a few 
raw_ASV_table_trimmed <- raw_ASV_table[,nchar(colnames(raw_ASV_table))  %in% 264:265]

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table_trimmed)))

# What proportion is left of the sequences? 
sum(raw_ASV_table_trimmed)/sum(raw_ASV_table)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length")

# Let's zoom in on the plot 
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length") + 
  scale_y_continuous(limits = c(0, 500))
```

# Remove Chimeras 

NOTE: Most of the reads should remain after the removal of chimeras 
```{r removing chimeras} 
# Remove the chimeras in the raw ASV table
noChimeras_ASV_table <- removeBimeraDenovo(raw_ASV_table_trimmed, 
                                           method="consensus", 
                                           multithread=TRUE, verbose=TRUE)

# Check the dimensions
dim(noChimeras_ASV_table)

# What proportion is left of the sequences? 
sum(noChimeras_ASV_table)/sum(raw_ASV_table_trimmed)
sum(noChimeras_ASV_table)/sum(raw_ASV_table)

# Plot it 
data.frame(Seq_Length_NoChim = nchar(getSequences(noChimeras_ASV_table))) %>%
  ggplot(aes(x = Seq_Length_NoChim )) + 
  geom_histogram()+ 
  labs(title = "Trimmed + Chimera Removal distribution of ASV length")
```

# Track read counts 

```{r}
# make function to identify number of sequence  
getN <- function(x) sum(getUniques(x))

# Make the table to track the seqs 
track <- cbind(filtered_reads, 
               sapply(dada_forward, getN),
               sapply(dada_reverse, getN),
               sapply(merged_ASVs, getN),
               rowSums(noChimeras_ASV_table))

head(track)

# Update column names to be more informative (most are missing at the moment!)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track) <- samples

# Generate a dataframe to track the reads through our DADA2 pipeline
track_counts_df <- 
  track %>%
  # make it a dataframe
  as.data.frame() %>%
  rownames_to_column(var = "names") %>%
  mutate(perc_reads_retained = 100 * nochim / input)

# Visualize it in table format 
DT::datatable(track_counts_df)

# Plot it!
track_counts_df %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type)) + 
  geom_line(aes(group = names), color = "grey") + 
  geom_point(shape = 21, size = 3, alpha = 0.8) + 
  scale_fill_brewer(palette = "Spectral") + 
  labs(x = "Filtering Step", y = "Number of Sequences") + 
  theme_bw()
```

# Assign Taxonomy 

```{r assign-taxonomy}
# Classify the ASVs against a reference set using the RDP Naive Bayesian Classifier described by Wang et al., (2007) in AEM
taxa_train <- 
  assignTaxonomy(noChimeras_ASV_table, 
                 "/workdir/in_class_data/taxonomy/silva_nr99_v138.1_train_set.fa.gz", 
                 multithread=TRUE)

# Add the genus/species information 
taxa_addSpecies <- 
  addSpecies(taxa_train, 
             "/workdir/in_class_data/taxonomy/silva_species_assignment_v138.1.fa.gz")

# Inspect the taxonomy 
taxa_print <- taxa_addSpecies # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
#View(taxa_print)
```

# Prepare data for export 

Below, we will prepare the following: 

## ASV table
1. Two ASV Count tables: 
      a. With ASV seqs: ASV headers include the *entire* ASV sequence ~250bps.
      b. with ASV names: This includes re-written and shortened headers like ASV_1, ASV_2, etc, which will match the names in our fasta file below.  
2. `ASV_fastas`: A fasta file that we can use to build a tree for phylogenetic analyses (e.g. phylogenetic alpha diversity metrics or UNIFRAC dissimilarty).


### Final ASV count table
```{r ASV-count-table}
## 2. COUNT TABLE 
##### Modify the ASV names and then save a fasta file! 
# Give headers more manageable names
asv_seqs <- colnames(noChimeras_ASV_table)
asv_seqs[1:5]

# make headers for our ASV seq fasta file, which will be our asv names
asv_headers <- vector(dim(noChimeras_ASV_table)[2], mode = "character")
asv_headers[1:5]

# loop through vector and fill it in with ASV names 
for (i in 1:dim(noChimeras_ASV_table)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep = "_")
}

#check
asv_headers[1:5]

## Rename ASVs in table then write out our ASV fasta file
asv_tab <- t(noChimeras_ASV_table)

## Rename our ASVs
row.names(asv_tab) <- sub(">", "", asv_headers)
```

## Taxonomy Table
```{r taxonomy-table}
# Inspect the taxonomy table
#View(taxa_addSpecies)

##### Prepare tax table 
# Add the ASV sequences from the rownames to a column 
new_tax_tab <- 
  taxa_addSpecies%>%
  as.data.frame() %>%
  rownames_to_column(var = "ASVseqs") 
head(new_tax_tab)

# intution check 
stopifnot(new_tax_tab$ASVseqs == colnames(noChimeras_ASV_table))

# Now let's add the ASV names 
rownames(new_tax_tab) <- rownames(asv_tab)
head(new_tax_tab)

### Final prep of tax table. Add new column with ASV names 
asv_tax <- 
  new_tax_tab %>%
  # add rownames from count table for phyloseq handoff
  mutate(ASV = rownames(asv_tab)) %>%
  # Resort the columns with select
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, Species, ASV, ASVseqs)

head(asv_tax)

# Intution check
stopifnot(asv_tax$ASV == rownames(asv_tax), rownames(asv_tax) == rownames(asv_tab))
```


```{r}
# FIRST, we will save our output as regular files, which will be useful later on. 
# Save to regular .tsv file 
# Write BOTH the modified and unmodified ASV tables to a file!
# Write count table with ASV numbered names (e.g. ASV_1, ASV_2, etc)
write.table(asv_tab, "data/01_DADA2/ASV_counts.tsv", sep = "\t", quote = FALSE, col.names = NA)
# Write count table with ASV sequence names
write.table(noChimeras_ASV_table, "data/01_DADA2/ASV_counts_withSeqNames.tsv", sep = "\t", quote = FALSE, col.names = NA)
# Write out the fasta file for reference later on for what seq matches what ASV
asv_fasta <- c(rbind(asv_headers, asv_seqs))
# Save to a file!
write(asv_fasta, "data/01_DADA2/ASVs.fasta")


# SECOND, let's save the taxonomy tables 
# Write the table 
write.table(asv_tax, "data/01_DADA2/ASV_taxonomy.tsv", sep = "\t", quote = FALSE, col.names = NA)


# THIRD, let's save to a RData object 
# Each of these files will be used in the analysis/02_Taxonomic_Assignment
# RData objects are for easy loading :) 
save(noChimeras_ASV_table, file = "data/01_DADA2/noChimeras_ASV_table.RData")
save(asv_tab, file = "data/01_DADA2/ASV_counts.RData")
# And save the track_counts_df a R object, which we will merge with metadata information in the next step of the analysis in nalysis/02_Taxonomic_Assignment. 
save(track_counts_df, file = "data/01_DADA2/track_read_counts.RData")
```

#Document Conclusions 
- In this document, I was able to successfully import raw fastq files and asses their quality, filter and trim reads based on QC plots, learn errors, infer ASVs and create a taxonomy table. A main take away is that filter and trimming is very important and has an impact on infering ASVs. I had to do quite a but if playing around with parameters to get a substantial ASV table.

# Check Render Time
```{r stop-time}
# Take the time now that we are at the end of the script
end_time <- Sys.time()
end_time 

# Echo the elapsed time
elapsed_time <- round((end_time - start_time), 3)
elapsed_time
```


# Session Information 
```{r session-info}
# Ensure reproducibility 
devtools::session_info()
```
